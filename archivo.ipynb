{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824025 sha256=1b89cd6f8a44b9ac4f6f8c3c46117003bf02d1ab2877d7be016dc9fee09cb970\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/89/d6/52/1178e354ba2207673484f0ccd7b2ded0ab6671ae5c1fc5b49a\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 11:43:00 WARN Utils: Your hostname, codespaces-571ad1 resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)\n",
      "23/02/20 11:43:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 11:43:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Creamos la sesion de spark\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "equip = pd.read_csv('equipo.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(equip, ['equipo', 'escudo', 'pais', 'puntuacion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+----------+----------+\n",
      "|        equipo|            escudo|      pais|puntuacion|\n",
      "+--------------+------------------+----------+----------+\n",
      "|      Atalanta|      atalanta.png|    Italia|         6|\n",
      "|      Atlético|      atletico.png|    España|        11|\n",
      "|     Barcelona|     barcelona.png|    España|        17|\n",
      "|        Bayern|        bayern.png|  Alemania|        16|\n",
      "|       Benfica|       benfica.png|  Portugal|        15|\n",
      "|      Besiktas|      besiktas.png|   Turquía|         2|\n",
      "|       Chelsea|       chelsea.png|Inglaterra|         1|\n",
      "|   Club Brugge|   club_brugge.png|   Bélgica|        26|\n",
      "|      Dortmund|      dortmund.png|  Alemania|        27|\n",
      "|   Dynamo Kyiv|   dynamo_kyiv.png|   Ucrania|        28|\n",
      "|Internazionale|internazionale.png|    Italia|        29|\n",
      "|      Juventus|      juventus.png|    Italia|         9|\n",
      "|       Leipzig|       leipzig.png|  Alemania|         8|\n",
      "|     Liverpool|     liverpool.png|Inglaterra|        24|\n",
      "|          LOSC|          losc.png|   Francia|        23|\n",
      "|         Malmö|         malmo.png|    Suecia|        22|\n",
      "|     Man. City|      man_city.png|Inglaterra|        21|\n",
      "|   Man. United|    man_united.png|Inglaterra|        20|\n",
      "|         Milan|         milan.png|    Italia|        19|\n",
      "|         Paris|         paris.png|   Francia|        14|\n",
      "+--------------+------------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['equipo', 'escudo', 'pais', 'puntuacion']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('equipo', 'string'),\n",
       " ('escudo', 'string'),\n",
       " ('pais', 'string'),\n",
       " ('puntuacion', 'bigint')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('equipo', StringType(), True), StructField('escudo', StringType(), True), StructField('pais', StringType(), True), StructField('puntuacion', LongType(), True)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- equipo: string (nullable = true)\n",
      " |-- escudo: string (nullable = true)\n",
      " |-- pais: string (nullable = true)\n",
      " |-- puntuacion: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+\n",
      "|        equipo|            escudo|\n",
      "+--------------+------------------+\n",
      "|      Atalanta|      atalanta.png|\n",
      "|      Atlético|      atletico.png|\n",
      "|     Barcelona|     barcelona.png|\n",
      "|        Bayern|        bayern.png|\n",
      "|       Benfica|       benfica.png|\n",
      "|      Besiktas|      besiktas.png|\n",
      "|       Chelsea|       chelsea.png|\n",
      "|   Club Brugge|   club_brugge.png|\n",
      "|      Dortmund|      dortmund.png|\n",
      "|   Dynamo Kyiv|   dynamo_kyiv.png|\n",
      "|Internazionale|internazionale.png|\n",
      "|      Juventus|      juventus.png|\n",
      "|       Leipzig|       leipzig.png|\n",
      "|     Liverpool|     liverpool.png|\n",
      "|          LOSC|          losc.png|\n",
      "|         Malmö|         malmo.png|\n",
      "|     Man. City|      man_city.png|\n",
      "|   Man. United|    man_united.png|\n",
      "|         Milan|         milan.png|\n",
      "|         Paris|         paris.png|\n",
      "+--------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"equipo\", \"escudo\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+----------+\n",
      "|      escudo|  pais|puntuacion|\n",
      "+------------+------+----------+\n",
      "|atalanta.png|Italia|         6|\n",
      "|atletico.png|España|        11|\n",
      "+------------+------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.drop(\"equipo\")\n",
    "newdf.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+----------+----------+\n",
      "|    equipo|        escudo|      pais|puntuacion|\n",
      "+----------+--------------+----------+----------+\n",
      "|   Chelsea|   chelsea.png|Inglaterra|         1|\n",
      "|  Besiktas|  besiktas.png|   Turquía|         2|\n",
      "| Wolfsburg| wolfsburg.png|  Alemania|         4|\n",
      "|  Atalanta|  atalanta.png|    Italia|         6|\n",
      "|Young Boys|young_boys.png|     Suiza|         7|\n",
      "+----------+--------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(\"puntuacion\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+--------+----------+\n",
      "|        equipo|            escudo|    pais|puntuacion|\n",
      "+--------------+------------------+--------+----------+\n",
      "|    Villarreal|    villarreal.png|  España|        30|\n",
      "|Internazionale|internazionale.png|  Italia|        29|\n",
      "|   Dynamo Kyiv|   dynamo_kyiv.png| Ucrania|        28|\n",
      "|      Dortmund|      dortmund.png|Alemania|        27|\n",
      "|   Club Brugge|   club_brugge.png| Bélgica|        26|\n",
      "+--------------+------------------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(desc(\"puntuacion\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = pd.read_csv('partidos.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "deptdf = spark.createDataFrame(par, ['fecha', 'id_local', 'id_visitante', 'pro_ganar','pro_empatar', 'pro_perder'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+------------+---------+-----------+----------+\n",
      "|           fecha|id_local|id_visitante|pro_ganar|pro_empatar|pro_perder|\n",
      "+----------------+--------+------------+---------+-----------+----------+\n",
      "|14/09/2021 16:45|      31|          19|       33|         34|        33|\n",
      "|14/09/2021 16:45|      25|          24|       33|         34|        33|\n",
      "|14/09/2021 19:00|      11|           6|       33|         34|        33|\n",
      "|14/09/2021 19:00|       4|           5|       33|         34|        33|\n",
      "|14/09/2021 19:00|      16|          30|       33|         34|        33|\n",
      "|14/09/2021 19:00|      17|          13|       33|         34|        33|\n",
      "|14/09/2021 19:00|       8|          32|       33|         34|        33|\n",
      "|15/09/2021 16:45|       7|          10|       33|         34|        33|\n",
      "|15/09/2021 16:45|      27|          26|       33|         34|        33|\n",
      "|15/09/2021 19:00|       9|          21|       33|         34|        33|\n",
      "|15/09/2021 19:00|      18|          14|       33|         34|        33|\n",
      "|15/09/2021 19:00|       3|          22|       33|         34|        33|\n",
      "|15/09/2021 19:00|      15|          20|       33|         34|        33|\n",
      "|15/09/2021 19:00|      28|           1|       33|         34|        33|\n",
      "|15/09/2021 19:00|      12|          23|       33|         34|        33|\n",
      "|28/09/2021 16:45|       1|           7|       33|         34|        33|\n",
      "|28/09/2021 16:45|      26|          12|       33|         34|        33|\n",
      "|28/09/2021 19:00|      21|          18|       33|         34|        33|\n",
      "|28/09/2021 19:00|      14|           9|       33|         34|        33|\n",
      "|28/09/2021 19:00|      22|          15|       33|         34|        33|\n",
      "+----------------+--------+------------+---------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deptdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+----------+----------+----------------+--------+------------+---------+-----------+----------+\n",
      "|    equipo|        escudo|      pais|puntuacion|           fecha|id_local|id_visitante|pro_ganar|pro_empatar|pro_perder|\n",
      "+----------+--------------+----------+----------+----------------+--------+------------+---------+-----------+----------+\n",
      "|   Chelsea|   chelsea.png|Inglaterra|         1|28/09/2021 16:45|       1|           7|       33|         34|        33|\n",
      "|   Chelsea|   chelsea.png|Inglaterra|         1|19/10/2021 19:00|       1|          10|       33|         34|        33|\n",
      "|   Chelsea|   chelsea.png|Inglaterra|         1|07/12/2021 20:00|       1|          28|       33|         34|        33|\n",
      "|  Besiktas|  besiktas.png|   Turquía|         2|29/09/2021 16:45|       2|          31|       33|         34|        33|\n",
      "|  Besiktas|  besiktas.png|   Turquía|         2|02/11/2021 20:00|       2|          19|       33|         34|        33|\n",
      "|  Besiktas|  besiktas.png|   Turquía|         2|08/12/2021 20:00|       2|          29|       33|         34|        33|\n",
      "| Wolfsburg| wolfsburg.png|  Alemania|         4|14/09/2021 19:00|       4|           5|       33|         34|        33|\n",
      "| Wolfsburg| wolfsburg.png|  Alemania|         4|20/10/2021 16:45|       4|          11|       33|         34|        33|\n",
      "| Wolfsburg| wolfsburg.png|  Alemania|         4|23/11/2021 20:00|       4|           6|       33|         34|        33|\n",
      "|  Atalanta|  atalanta.png|    Italia|         6|29/09/2021 19:00|       6|           4|       33|         34|        33|\n",
      "|  Atalanta|  atalanta.png|    Italia|         6|20/10/2021 19:00|       6|           5|       33|         34|        33|\n",
      "|  Atalanta|  atalanta.png|    Italia|         6|08/12/2021 20:00|       6|          11|       33|         34|        33|\n",
      "|Young Boys|young_boys.png|     Suiza|         7|15/09/2021 16:45|       7|          10|       33|         34|        33|\n",
      "|Young Boys|young_boys.png|     Suiza|         7|19/10/2021 16:45|       7|          28|       33|         34|        33|\n",
      "|Young Boys|young_boys.png|     Suiza|         7|24/11/2021 17:45|       7|           1|       33|         34|        33|\n",
      "|   Leipzig|   leipzig.png|  Alemania|         8|14/09/2021 19:00|       8|          32|       33|         34|        33|\n",
      "|   Leipzig|   leipzig.png|  Alemania|         8|20/10/2021 19:00|       8|          17|       33|         34|        33|\n",
      "|   Leipzig|   leipzig.png|  Alemania|         8|23/11/2021 20:00|       8|          13|       33|         34|        33|\n",
      "|  Juventus|  juventus.png|    Italia|         9|15/09/2021 19:00|       9|          21|       33|         34|        33|\n",
      "|  Juventus|  juventus.png|    Italia|         9|19/10/2021 16:45|       9|          18|       33|         34|        33|\n",
      "+----------+--------------+----------+----------+----------------+--------+------------+---------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.join(deptdf, df[\"puntuacion\"] == deptdf[\"id_local\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+--------+----------+----------------+--------+------------+---------+-----------+----------+\n",
      "|        equipo|            escudo|    pais|puntuacion|           fecha|id_local|id_visitante|pro_ganar|pro_empatar|pro_perder|\n",
      "+--------------+------------------+--------+----------+----------------+--------+------------+---------+-----------+----------+\n",
      "|   Club Brugge|   club_brugge.png| Bélgica|        26|07/12/2021 20:00|      26|          27|       33|         34|        33|\n",
      "|   Club Brugge|   club_brugge.png| Bélgica|        26|19/10/2021 19:00|      26|          23|       33|         34|        33|\n",
      "|   Club Brugge|   club_brugge.png| Bélgica|        26|28/09/2021 16:45|      26|          12|       33|         34|        33|\n",
      "|Internazionale|internazionale.png|  Italia|        29|23/11/2021 17:45|      29|          19|       33|         34|        33|\n",
      "|Internazionale|internazionale.png|  Italia|        29|02/11/2021 20:00|      29|          31|       33|         34|        33|\n",
      "|      Atalanta|      atalanta.png|  Italia|         6|08/12/2021 20:00|       6|          11|       33|         34|        33|\n",
      "|      Atalanta|      atalanta.png|  Italia|         6|20/10/2021 19:00|       6|           5|       33|         34|        33|\n",
      "|      Atalanta|      atalanta.png|  Italia|         6|29/09/2021 19:00|       6|           4|       33|         34|        33|\n",
      "|      Juventus|      juventus.png|  Italia|         9|24/11/2021 20:00|       9|          14|       33|         34|        33|\n",
      "|      Juventus|      juventus.png|  Italia|         9|19/10/2021 16:45|       9|          18|       33|         34|        33|\n",
      "|      Juventus|      juventus.png|  Italia|         9|15/09/2021 19:00|       9|          21|       33|         34|        33|\n",
      "|      Dortmund|      dortmund.png|Alemania|        27|24/11/2021 20:00|      27|          23|       33|         34|        33|\n",
      "|      Dortmund|      dortmund.png|Alemania|        27|03/11/2021 20:00|      27|          12|       33|         34|        33|\n",
      "|      Dortmund|      dortmund.png|Alemania|        27|15/09/2021 16:45|      27|          26|       33|         34|        33|\n",
      "|     Barcelona|     barcelona.png|  España|        17|23/11/2021 20:00|      17|          32|       33|         34|        33|\n",
      "|     Barcelona|     barcelona.png|  España|        17|02/11/2021 17:45|      17|           8|       33|         34|        33|\n",
      "|     Barcelona|     barcelona.png|  España|        17|14/09/2021 19:00|      17|          13|       33|         34|        33|\n",
      "|   Dynamo Kyiv|   dynamo_kyiv.png| Ucrania|        28|24/11/2021 20:00|      28|          10|       33|         34|        33|\n",
      "|   Dynamo Kyiv|   dynamo_kyiv.png| Ucrania|        28|03/11/2021 20:00|      28|           7|       33|         34|        33|\n",
      "|   Dynamo Kyiv|   dynamo_kyiv.png| Ucrania|        28|15/09/2021 19:00|      28|           1|       33|         34|        33|\n",
      "+--------------+------------------+--------+----------+----------------+--------+------------+---------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(deptdf, df[\"puntuacion\"] == deptdf[\"id_local\"], \"left_outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+----------+----------+----------------+--------+------------+---------+-----------+----------+\n",
      "|          equipo|              escudo|      pais|puntuacion|           fecha|id_local|id_visitante|pro_ganar|pro_empatar|pro_perder|\n",
      "+----------------+--------------------+----------+----------+----------------+--------+------------+---------+-----------+----------+\n",
      "|     Club Brugge|     club_brugge.png|   Bélgica|        26|28/09/2021 16:45|      26|          12|       33|         34|        33|\n",
      "|         Sheriff|         sheriff.png|  Moldavia|        22|28/09/2021 19:00|      22|          15|       33|         34|        33|\n",
      "|           Malmö|           malmo.png|    Suecia|        22|28/09/2021 19:00|      22|          15|       33|         34|        33|\n",
      "|      Young Boys|      young_boys.png|     Suiza|         7|15/09/2021 16:45|       7|          10|       33|         34|        33|\n",
      "|            null|                null|      null|      null|14/09/2021 16:45|      31|          19|       33|         34|        33|\n",
      "|     Sporting CP|     sporting_cp.png|  Portugal|        25|14/09/2021 16:45|      25|          24|       33|         34|        33|\n",
      "|        Juventus|        juventus.png|    Italia|         9|15/09/2021 19:00|       9|          21|       33|         34|        33|\n",
      "|        Dortmund|        dortmund.png|  Alemania|        27|15/09/2021 16:45|      27|          26|       33|         34|        33|\n",
      "|       Barcelona|       barcelona.png|    España|        17|14/09/2021 19:00|      17|          13|       33|         34|        33|\n",
      "|     Dynamo Kyiv|     dynamo_kyiv.png|   Ucrania|        28|15/09/2021 19:00|      28|           1|       33|         34|        33|\n",
      "|         Chelsea|         chelsea.png|Inglaterra|         1|28/09/2021 16:45|       1|           7|       33|         34|        33|\n",
      "|            null|                null|      null|      null|15/09/2021 19:00|       3|          22|       33|         34|        33|\n",
      "|     Real Madrid|     real_madrid.png|    España|        12|15/09/2021 19:00|      12|          23|       33|         34|        33|\n",
      "|         Leipzig|         leipzig.png|  Alemania|         8|14/09/2021 19:00|       8|          32|       33|         34|        33|\n",
      "|        Salzburg|        salzburg.png|   Austria|        11|14/09/2021 19:00|      11|           6|       33|         34|        33|\n",
      "|        Atlético|        atletico.png|    España|        11|14/09/2021 19:00|      11|           6|       33|         34|        33|\n",
      "|       Wolfsburg|       wolfsburg.png|  Alemania|         4|14/09/2021 19:00|       4|           5|       33|         34|        33|\n",
      "|            null|                null|      null|      null|15/09/2021 19:00|      18|          14|       33|         34|        33|\n",
      "|Shakhtar Donetsk|shakhtar_donetsk.png|   Ucrania|        14|28/09/2021 19:00|      14|           9|       33|         34|        33|\n",
      "|           Paris|           paris.png|   Francia|        14|28/09/2021 19:00|      14|           9|       33|         34|        33|\n",
      "+----------------+--------------------+----------+----------+----------------+--------+------------+---------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.join(deptdf, df[\"puntuacion\"] == deptdf[\"id_local\"], \"right_outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+----------+----------+----------------+--------+------------+---------+-----------+----------+\n",
      "|    equipo|        escudo|      pais|puntuacion|           fecha|id_local|id_visitante|pro_ganar|pro_empatar|pro_perder|\n",
      "+----------+--------------+----------+----------+----------------+--------+------------+---------+-----------+----------+\n",
      "|   Chelsea|   chelsea.png|Inglaterra|         1|28/09/2021 16:45|       1|           7|       33|         34|        33|\n",
      "|   Chelsea|   chelsea.png|Inglaterra|         1|19/10/2021 19:00|       1|          10|       33|         34|        33|\n",
      "|   Chelsea|   chelsea.png|Inglaterra|         1|07/12/2021 20:00|       1|          28|       33|         34|        33|\n",
      "|  Besiktas|  besiktas.png|   Turquía|         2|29/09/2021 16:45|       2|          31|       33|         34|        33|\n",
      "|  Besiktas|  besiktas.png|   Turquía|         2|02/11/2021 20:00|       2|          19|       33|         34|        33|\n",
      "|  Besiktas|  besiktas.png|   Turquía|         2|08/12/2021 20:00|       2|          29|       33|         34|        33|\n",
      "|      null|          null|      null|      null|15/09/2021 19:00|       3|          22|       33|         34|        33|\n",
      "|      null|          null|      null|      null|19/10/2021 19:00|       3|          15|       33|         34|        33|\n",
      "|      null|          null|      null|      null|24/11/2021 20:00|       3|          20|       33|         34|        33|\n",
      "| Wolfsburg| wolfsburg.png|  Alemania|         4|14/09/2021 19:00|       4|           5|       33|         34|        33|\n",
      "| Wolfsburg| wolfsburg.png|  Alemania|         4|20/10/2021 16:45|       4|          11|       33|         34|        33|\n",
      "| Wolfsburg| wolfsburg.png|  Alemania|         4|23/11/2021 20:00|       4|           6|       33|         34|        33|\n",
      "|      null|          null|      null|      null|29/09/2021 19:00|       5|          11|       33|         34|        33|\n",
      "|      null|          null|      null|      null|02/11/2021 20:00|       5|           6|       33|         34|        33|\n",
      "|      null|          null|      null|      null|08/12/2021 20:00|       5|           4|       33|         34|        33|\n",
      "|  Atalanta|  atalanta.png|    Italia|         6|29/09/2021 19:00|       6|           4|       33|         34|        33|\n",
      "|  Atalanta|  atalanta.png|    Italia|         6|20/10/2021 19:00|       6|           5|       33|         34|        33|\n",
      "|  Atalanta|  atalanta.png|    Italia|         6|08/12/2021 20:00|       6|          11|       33|         34|        33|\n",
      "|Young Boys|young_boys.png|     Suiza|         7|15/09/2021 16:45|       7|          10|       33|         34|        33|\n",
      "|Young Boys|young_boys.png|     Suiza|         7|19/10/2021 16:45|       7|          28|       33|         34|        33|\n",
      "+----------+--------------+----------+----------+----------------+--------+------------+---------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(deptdf, df[\"puntuacion\"] == deptdf[\"id_local\"], \"outer\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------+----------+\n",
      "| equipo|     escudo|      pais|puntuacion|\n",
      "+-------+-----------+----------+----------+\n",
      "|Chelsea|chelsea.png|Inglaterra|         1|\n",
      "+-------+-----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register DataFrame as Temporary Table\n",
    "df.createOrReplaceTempView(\"temp_table\")\n",
    "\n",
    "# Execute SQL-Like query.\n",
    "spark.sql(\"select * from temp_table where puntuacion = 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|puntuacion|\n",
      "+----------+\n",
      "|        26|\n",
      "|        29|\n",
      "|         6|\n",
      "|         9|\n",
      "|        27|\n",
      "|        17|\n",
      "|        28|\n",
      "|         1|\n",
      "|         8|\n",
      "|        11|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select distinct puntuacion from temp_table\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3900709972.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[40], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    df = spark.table(\"DB_NAME\".\"TBL_NAME\")\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# DB_NAME : Name of the the HIVE Database\n",
    "# TBL_NAME : Name of the HIVE Table\n",
    "\n",
    "\n",
    "df = spark.table(\"DB_NAME\".\"TBL_NAME\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No me deja hacer estas cosas por error de sintasys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos con la segunda parte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create Temp Tables\n",
    "df.createOrReplaceTempView(\"empdf\")\n",
    "deptdf.createOrReplaceTempView(\"deptdf\")\n",
    "\n",
    "# Save as HIVE tables.\n",
    "df.write.saveAsTable(\"hive_empdf\", mode = \"overwrite\")\n",
    "deptdf.write.saveAsTable(\"hive_deptdf\", mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 50 * 1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Used : True\n",
      "Disk Used : True\n"
     ]
    }
   ],
   "source": [
    "df.cache()\n",
    "df.count()\n",
    "print(\"Memory Used : {0}\".format(df.storageLevel.useMemory))\n",
    "print(\"Disk Used : {0}\".format(df.storageLevel.useDisk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[equipo: string, escudo: string, pais: string, puntuacion: bigint]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sqlContext' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sqlContext\u001b[39m.\u001b[39mclearCache()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sqlContext' is not defined"
     ]
    }
   ],
   "source": [
    "sqlContext.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+----------+----------+----------------+\n",
      "|        equipo|            escudo|      pais|puntuacion|puntuacion_level|\n",
      "+--------------+------------------+----------+----------+----------------+\n",
      "|      Atalanta|      atalanta.png|    Italia|         6| puntuacion_baja|\n",
      "|      Atlético|      atletico.png|    España|        11| puntuacion_baja|\n",
      "|     Barcelona|     barcelona.png|    España|        17|puntuacion_media|\n",
      "|        Bayern|        bayern.png|  Alemania|        16|puntuacion_media|\n",
      "|       Benfica|       benfica.png|  Portugal|        15| puntuacion_baja|\n",
      "|      Besiktas|      besiktas.png|   Turquía|         2| puntuacion_baja|\n",
      "|       Chelsea|       chelsea.png|Inglaterra|         1| puntuacion_baja|\n",
      "|   Club Brugge|   club_brugge.png|   Bélgica|        26|puntuacion_media|\n",
      "|      Dortmund|      dortmund.png|  Alemania|        27|puntuacion_media|\n",
      "|   Dynamo Kyiv|   dynamo_kyiv.png|   Ucrania|        28|puntuacion_media|\n",
      "|Internazionale|internazionale.png|    Italia|        29| puntuacion_alta|\n",
      "|      Juventus|      juventus.png|    Italia|         9| puntuacion_baja|\n",
      "|       Leipzig|       leipzig.png|  Alemania|         8| puntuacion_baja|\n",
      "|     Liverpool|     liverpool.png|Inglaterra|        24|puntuacion_media|\n",
      "|          LOSC|          losc.png|   Francia|        23|puntuacion_media|\n",
      "|         Malmö|         malmo.png|    Suecia|        22|puntuacion_media|\n",
      "|     Man. City|      man_city.png|Inglaterra|        21|puntuacion_media|\n",
      "|   Man. United|    man_united.png|Inglaterra|        20|puntuacion_media|\n",
      "|         Milan|         milan.png|    Italia|        19|puntuacion_media|\n",
      "|         Paris|         paris.png|   Francia|        14| puntuacion_baja|\n",
      "+--------------+------------------+----------+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "\n",
    "\n",
    "cond = \"\"\"case when puntuacion> 28 then 'puntuacion_alta'\n",
    "               else case when puntuacion > 15 then 'puntuacion_media'\n",
    "                    else case when puntuacion > 0 then 'puntuacion_baja'\n",
    "                         else 'invalid_puntuacion'\n",
    "                              end\n",
    "                         end\n",
    "                end as puntuacion_level\"\"\"\n",
    "\n",
    "newdf = df.withColumn(\"puntuacion_level\", expr(cond))\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+----------+----------+----------------+\n",
      "|        equipo|            escudo|      pais|puntuacion|puntuacion_level|\n",
      "+--------------+------------------+----------+----------+----------------+\n",
      "|      Atalanta|      atalanta.png|    Italia|         6| puntuacion_baja|\n",
      "|      Atlético|      atletico.png|    España|        11| puntuacion_baja|\n",
      "|     Barcelona|     barcelona.png|    España|        17|puntuacion_media|\n",
      "|        Bayern|        bayern.png|  Alemania|        16|puntuacion_media|\n",
      "|       Benfica|       benfica.png|  Portugal|        15| puntuacion_baja|\n",
      "|      Besiktas|      besiktas.png|   Turquía|         2| puntuacion_baja|\n",
      "|       Chelsea|       chelsea.png|Inglaterra|         1| puntuacion_baja|\n",
      "|   Club Brugge|   club_brugge.png|   Bélgica|        26|puntuacion_media|\n",
      "|      Dortmund|      dortmund.png|  Alemania|        27|puntuacion_media|\n",
      "|   Dynamo Kyiv|   dynamo_kyiv.png|   Ucrania|        28|puntuacion_media|\n",
      "|Internazionale|internazionale.png|    Italia|        29| puntuacion_alta|\n",
      "|      Juventus|      juventus.png|    Italia|         9| puntuacion_baja|\n",
      "|       Leipzig|       leipzig.png|  Alemania|         8| puntuacion_baja|\n",
      "|     Liverpool|     liverpool.png|Inglaterra|        24|puntuacion_media|\n",
      "|          LOSC|          losc.png|   Francia|        23|puntuacion_media|\n",
      "|         Malmö|         malmo.png|    Suecia|        22|puntuacion_media|\n",
      "|     Man. City|      man_city.png|Inglaterra|        21|puntuacion_media|\n",
      "|   Man. United|    man_united.png|Inglaterra|        20|puntuacion_media|\n",
      "|         Milan|         milan.png|    Italia|        19|puntuacion_media|\n",
      "|         Paris|         paris.png|   Francia|        14| puntuacion_baja|\n",
      "+--------------+------------------+----------+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.selectExpr(\"*\", cond)\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+----------+----------+\n",
      "|        equipo|            escudo|      pais|puntuacion|\n",
      "+--------------+------------------+----------+----------+\n",
      "|      Atalanta|      atalanta.png|    Italia|         6|\n",
      "|      Atlético|      atletico.png|    España|        11|\n",
      "|     Barcelona|     barcelona.png|    España|        17|\n",
      "|        Bayern|        bayern.png|  Alemania|        16|\n",
      "|       Benfica|       benfica.png|  Portugal|        15|\n",
      "|      Besiktas|      besiktas.png|   Turquía|         2|\n",
      "|       Chelsea|       chelsea.png|Inglaterra|         1|\n",
      "|   Club Brugge|   club_brugge.png|   Bélgica|        26|\n",
      "|      Dortmund|      dortmund.png|  Alemania|        27|\n",
      "|   Dynamo Kyiv|   dynamo_kyiv.png|   Ucrania|        28|\n",
      "|Internazionale|internazionale.png|    Italia|        29|\n",
      "|      Juventus|      juventus.png|    Italia|         9|\n",
      "|       Leipzig|       leipzig.png|  Alemania|         8|\n",
      "|     Liverpool|     liverpool.png|Inglaterra|        24|\n",
      "|          LOSC|          losc.png|   Francia|        23|\n",
      "|         Malmö|         malmo.png|    Suecia|        22|\n",
      "|     Man. City|      man_city.png|Inglaterra|        21|\n",
      "|   Man. United|    man_united.png|Inglaterra|        20|\n",
      "|         Milan|         milan.png|    Italia|        19|\n",
      "|         Paris|         paris.png|   Francia|        14|\n",
      "+--------------+------------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.dropna()\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+----------+----------+\n",
      "|        equipo|            escudo|      pais|puntuacion|\n",
      "+--------------+------------------+----------+----------+\n",
      "|      Atalanta|      atalanta.png|    Italia|         6|\n",
      "|      Atlético|      atletico.png|    España|        11|\n",
      "|     Barcelona|     barcelona.png|    España|        17|\n",
      "|        Bayern|        bayern.png|  Alemania|        16|\n",
      "|       Benfica|       benfica.png|  Portugal|        15|\n",
      "|      Besiktas|      besiktas.png|   Turquía|         2|\n",
      "|       Chelsea|       chelsea.png|Inglaterra|         1|\n",
      "|   Club Brugge|   club_brugge.png|   Bélgica|        26|\n",
      "|      Dortmund|      dortmund.png|  Alemania|        27|\n",
      "|   Dynamo Kyiv|   dynamo_kyiv.png|   Ucrania|        28|\n",
      "|Internazionale|internazionale.png|    Italia|        29|\n",
      "|      Juventus|      juventus.png|    Italia|         9|\n",
      "|       Leipzig|       leipzig.png|  Alemania|         8|\n",
      "|     Liverpool|     liverpool.png|Inglaterra|        24|\n",
      "|          LOSC|          losc.png|   Francia|        23|\n",
      "|         Malmö|         malmo.png|    Suecia|        22|\n",
      "|     Man. City|      man_city.png|Inglaterra|        21|\n",
      "|   Man. United|    man_united.png|Inglaterra|        20|\n",
      "|         Milan|         milan.png|    Italia|        19|\n",
      "|         Paris|         paris.png|   Francia|        14|\n",
      "+--------------+------------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Elimina todas las filas que contienen todos los valores nulos.\n",
    "newdf = df.dropna(how = \"all\")\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Numero de participaciones\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#incremente el numero de particiones\n",
    "newdf = df.repartition(6)\n",
    "newdf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#decremente el numero de particiones\n",
    "newdf = df.coalesce(2)\n",
    "newdf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Partitions : 500\n"
     ]
    }
   ],
   "source": [
    "# Set number of partitions as Spark Application.\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"500\")\n",
    "\n",
    "# Check the number of patitions.\n",
    "num_part = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "print(\"No of Partitions : {0}\".format(num_part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='default database', locationUri='file:/workspaces/Apache_Spark/spark-warehouse')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#devolver  las bases de datos\n",
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='hive_deptdf', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='hive_empdf', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='deptdf', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='empdf', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='temp_table', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#devolver las tablas\n",
    "spark.catalog.listTables(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='equipo', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='escudo', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='pais', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='puntuacion', description=None, dataType='bigint', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#devolvera laa columnas de la tabla\n",
    "spark.catalog.listColumns(\"hive_empdf\", \"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Function(name='!', description=None, className='org.apache.spark.sql.catalyst.expressions.Not', isTemporary=True),\n",
       " Function(name='%', description=None, className='org.apache.spark.sql.catalyst.expressions.Remainder', isTemporary=True),\n",
       " Function(name='&', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseAnd', isTemporary=True),\n",
       " Function(name='*', description=None, className='org.apache.spark.sql.catalyst.expressions.Multiply', isTemporary=True),\n",
       " Function(name='+', description=None, className='org.apache.spark.sql.catalyst.expressions.Add', isTemporary=True),\n",
       " Function(name='-', description=None, className='org.apache.spark.sql.catalyst.expressions.Subtract', isTemporary=True),\n",
       " Function(name='/', description=None, className='org.apache.spark.sql.catalyst.expressions.Divide', isTemporary=True),\n",
       " Function(name='<', description=None, className='org.apache.spark.sql.catalyst.expressions.LessThan', isTemporary=True),\n",
       " Function(name='<=', description=None, className='org.apache.spark.sql.catalyst.expressions.LessThanOrEqual', isTemporary=True),\n",
       " Function(name='<=>', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualNullSafe', isTemporary=True),\n",
       " Function(name='=', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualTo', isTemporary=True),\n",
       " Function(name='==', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualTo', isTemporary=True),\n",
       " Function(name='>', description=None, className='org.apache.spark.sql.catalyst.expressions.GreaterThan', isTemporary=True),\n",
       " Function(name='>=', description=None, className='org.apache.spark.sql.catalyst.expressions.GreaterThanOrEqual', isTemporary=True),\n",
       " Function(name='^', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseXor', isTemporary=True),\n",
       " Function(name='abs', description=None, className='org.apache.spark.sql.catalyst.expressions.Abs', isTemporary=True),\n",
       " Function(name='acos', description=None, className='org.apache.spark.sql.catalyst.expressions.Acos', isTemporary=True),\n",
       " Function(name='acosh', description=None, className='org.apache.spark.sql.catalyst.expressions.Acosh', isTemporary=True),\n",
       " Function(name='add_months', description=None, className='org.apache.spark.sql.catalyst.expressions.AddMonths', isTemporary=True),\n",
       " Function(name='aes_decrypt', description=None, className='org.apache.spark.sql.catalyst.expressions.AesDecrypt', isTemporary=True),\n",
       " Function(name='aes_encrypt', description=None, className='org.apache.spark.sql.catalyst.expressions.AesEncrypt', isTemporary=True),\n",
       " Function(name='aggregate', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayAggregate', isTemporary=True),\n",
       " Function(name='and', description=None, className='org.apache.spark.sql.catalyst.expressions.And', isTemporary=True),\n",
       " Function(name='any', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True),\n",
       " Function(name='approx_count_distinct', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.HyperLogLogPlusPlus', isTemporary=True),\n",
       " Function(name='approx_percentile', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile', isTemporary=True),\n",
       " Function(name='array', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateArray', isTemporary=True),\n",
       " Function(name='array_agg', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CollectList', isTemporary=True),\n",
       " Function(name='array_contains', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayContains', isTemporary=True),\n",
       " Function(name='array_distinct', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayDistinct', isTemporary=True),\n",
       " Function(name='array_except', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayExcept', isTemporary=True),\n",
       " Function(name='array_intersect', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayIntersect', isTemporary=True),\n",
       " Function(name='array_join', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayJoin', isTemporary=True),\n",
       " Function(name='array_max', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayMax', isTemporary=True),\n",
       " Function(name='array_min', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayMin', isTemporary=True),\n",
       " Function(name='array_position', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayPosition', isTemporary=True),\n",
       " Function(name='array_remove', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayRemove', isTemporary=True),\n",
       " Function(name='array_repeat', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayRepeat', isTemporary=True),\n",
       " Function(name='array_size', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraySize', isTemporary=True),\n",
       " Function(name='array_sort', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraySort', isTemporary=True),\n",
       " Function(name='array_union', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayUnion', isTemporary=True),\n",
       " Function(name='arrays_overlap', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraysOverlap', isTemporary=True),\n",
       " Function(name='arrays_zip', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraysZip', isTemporary=True),\n",
       " Function(name='ascii', description=None, className='org.apache.spark.sql.catalyst.expressions.Ascii', isTemporary=True),\n",
       " Function(name='asin', description=None, className='org.apache.spark.sql.catalyst.expressions.Asin', isTemporary=True),\n",
       " Function(name='asinh', description=None, className='org.apache.spark.sql.catalyst.expressions.Asinh', isTemporary=True),\n",
       " Function(name='assert_true', description=None, className='org.apache.spark.sql.catalyst.expressions.AssertTrue', isTemporary=True),\n",
       " Function(name='atan', description=None, className='org.apache.spark.sql.catalyst.expressions.Atan', isTemporary=True),\n",
       " Function(name='atan2', description=None, className='org.apache.spark.sql.catalyst.expressions.Atan2', isTemporary=True),\n",
       " Function(name='atanh', description=None, className='org.apache.spark.sql.catalyst.expressions.Atanh', isTemporary=True),\n",
       " Function(name='avg', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Average', isTemporary=True),\n",
       " Function(name='base64', description=None, className='org.apache.spark.sql.catalyst.expressions.Base64', isTemporary=True),\n",
       " Function(name='bigint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='bin', description=None, className='org.apache.spark.sql.catalyst.expressions.Bin', isTemporary=True),\n",
       " Function(name='binary', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='bit_and', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BitAndAgg', isTemporary=True),\n",
       " Function(name='bit_count', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseCount', isTemporary=True),\n",
       " Function(name='bit_get', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseGet', isTemporary=True),\n",
       " Function(name='bit_length', description=None, className='org.apache.spark.sql.catalyst.expressions.BitLength', isTemporary=True),\n",
       " Function(name='bit_or', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BitOrAgg', isTemporary=True),\n",
       " Function(name='bit_xor', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BitXorAgg', isTemporary=True),\n",
       " Function(name='bool_and', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolAnd', isTemporary=True),\n",
       " Function(name='bool_or', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True),\n",
       " Function(name='boolean', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='bround', description=None, className='org.apache.spark.sql.catalyst.expressions.BRound', isTemporary=True),\n",
       " Function(name='btrim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrimBoth', isTemporary=True),\n",
       " Function(name='cardinality', description=None, className='org.apache.spark.sql.catalyst.expressions.Size', isTemporary=True),\n",
       " Function(name='cast', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='cbrt', description=None, className='org.apache.spark.sql.catalyst.expressions.Cbrt', isTemporary=True),\n",
       " Function(name='ceil', description=None, className='org.apache.spark.sql.catalyst.expressions.CeilExpressionBuilder', isTemporary=True),\n",
       " Function(name='ceiling', description=None, className='org.apache.spark.sql.catalyst.expressions.CeilExpressionBuilder', isTemporary=True),\n",
       " Function(name='char', description=None, className='org.apache.spark.sql.catalyst.expressions.Chr', isTemporary=True),\n",
       " Function(name='char_length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
       " Function(name='character_length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
       " Function(name='chr', description=None, className='org.apache.spark.sql.catalyst.expressions.Chr', isTemporary=True),\n",
       " Function(name='coalesce', description=None, className='org.apache.spark.sql.catalyst.expressions.Coalesce', isTemporary=True),\n",
       " Function(name='collect_list', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CollectList', isTemporary=True),\n",
       " Function(name='collect_set', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CollectSet', isTemporary=True),\n",
       " Function(name='concat', description=None, className='org.apache.spark.sql.catalyst.expressions.Concat', isTemporary=True),\n",
       " Function(name='concat_ws', description=None, className='org.apache.spark.sql.catalyst.expressions.ConcatWs', isTemporary=True),\n",
       " Function(name='contains', description=None, className='org.apache.spark.sql.catalyst.expressions.ContainsExpressionBuilder', isTemporary=True),\n",
       " Function(name='conv', description=None, className='org.apache.spark.sql.catalyst.expressions.Conv', isTemporary=True),\n",
       " Function(name='corr', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Corr', isTemporary=True),\n",
       " Function(name='cos', description=None, className='org.apache.spark.sql.catalyst.expressions.Cos', isTemporary=True),\n",
       " Function(name='cosh', description=None, className='org.apache.spark.sql.catalyst.expressions.Cosh', isTemporary=True),\n",
       " Function(name='cot', description=None, className='org.apache.spark.sql.catalyst.expressions.Cot', isTemporary=True),\n",
       " Function(name='count', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Count', isTemporary=True),\n",
       " Function(name='count_if', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CountIf', isTemporary=True),\n",
       " Function(name='count_min_sketch', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CountMinSketchAgg', isTemporary=True),\n",
       " Function(name='covar_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CovPopulation', isTemporary=True),\n",
       " Function(name='covar_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CovSample', isTemporary=True),\n",
       " Function(name='crc32', description=None, className='org.apache.spark.sql.catalyst.expressions.Crc32', isTemporary=True),\n",
       " Function(name='csc', description=None, className='org.apache.spark.sql.catalyst.expressions.Csc', isTemporary=True),\n",
       " Function(name='cume_dist', description=None, className='org.apache.spark.sql.catalyst.expressions.CumeDist', isTemporary=True),\n",
       " Function(name='current_catalog', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentCatalog', isTemporary=True),\n",
       " Function(name='current_database', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentDatabase', isTemporary=True),\n",
       " Function(name='current_date', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentDate', isTemporary=True),\n",
       " Function(name='current_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentTimestamp', isTemporary=True),\n",
       " Function(name='current_timezone', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentTimeZone', isTemporary=True),\n",
       " Function(name='current_user', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentUser', isTemporary=True),\n",
       " Function(name='date', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='date_add', description=None, className='org.apache.spark.sql.catalyst.expressions.DateAdd', isTemporary=True),\n",
       " Function(name='date_format', description=None, className='org.apache.spark.sql.catalyst.expressions.DateFormatClass', isTemporary=True),\n",
       " Function(name='date_from_unix_date', description=None, className='org.apache.spark.sql.catalyst.expressions.DateFromUnixDate', isTemporary=True),\n",
       " Function(name='date_part', description=None, className='org.apache.spark.sql.catalyst.expressions.DatePartExpressionBuilder', isTemporary=True),\n",
       " Function(name='date_sub', description=None, className='org.apache.spark.sql.catalyst.expressions.DateSub', isTemporary=True),\n",
       " Function(name='date_trunc', description=None, className='org.apache.spark.sql.catalyst.expressions.TruncTimestamp', isTemporary=True),\n",
       " Function(name='datediff', description=None, className='org.apache.spark.sql.catalyst.expressions.DateDiff', isTemporary=True),\n",
       " Function(name='day', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfMonth', isTemporary=True),\n",
       " Function(name='dayofmonth', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfMonth', isTemporary=True),\n",
       " Function(name='dayofweek', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfWeek', isTemporary=True),\n",
       " Function(name='dayofyear', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfYear', isTemporary=True),\n",
       " Function(name='decimal', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='decode', description=None, className='org.apache.spark.sql.catalyst.expressions.Decode', isTemporary=True),\n",
       " Function(name='degrees', description=None, className='org.apache.spark.sql.catalyst.expressions.ToDegrees', isTemporary=True),\n",
       " Function(name='dense_rank', description=None, className='org.apache.spark.sql.catalyst.expressions.DenseRank', isTemporary=True),\n",
       " Function(name='div', description=None, className='org.apache.spark.sql.catalyst.expressions.IntegralDivide', isTemporary=True),\n",
       " Function(name='double', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='e', description=None, className='org.apache.spark.sql.catalyst.expressions.EulerNumber', isTemporary=True),\n",
       " Function(name='element_at', description=None, className='org.apache.spark.sql.catalyst.expressions.ElementAt', isTemporary=True),\n",
       " Function(name='elt', description=None, className='org.apache.spark.sql.catalyst.expressions.Elt', isTemporary=True),\n",
       " Function(name='encode', description=None, className='org.apache.spark.sql.catalyst.expressions.Encode', isTemporary=True),\n",
       " Function(name='endswith', description=None, className='org.apache.spark.sql.catalyst.expressions.EndsWithExpressionBuilder', isTemporary=True),\n",
       " Function(name='every', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolAnd', isTemporary=True),\n",
       " Function(name='exists', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayExists', isTemporary=True),\n",
       " Function(name='exp', description=None, className='org.apache.spark.sql.catalyst.expressions.Exp', isTemporary=True),\n",
       " Function(name='explode', description=None, className='org.apache.spark.sql.catalyst.expressions.Explode', isTemporary=True),\n",
       " Function(name='explode_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.Explode', isTemporary=True),\n",
       " Function(name='expm1', description=None, className='org.apache.spark.sql.catalyst.expressions.Expm1', isTemporary=True),\n",
       " Function(name='extract', description=None, className='org.apache.spark.sql.catalyst.expressions.Extract', isTemporary=True),\n",
       " Function(name='factorial', description=None, className='org.apache.spark.sql.catalyst.expressions.Factorial', isTemporary=True),\n",
       " Function(name='filter', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayFilter', isTemporary=True),\n",
       " Function(name='find_in_set', description=None, className='org.apache.spark.sql.catalyst.expressions.FindInSet', isTemporary=True),\n",
       " Function(name='first', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.First', isTemporary=True),\n",
       " Function(name='first_value', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.First', isTemporary=True),\n",
       " Function(name='flatten', description=None, className='org.apache.spark.sql.catalyst.expressions.Flatten', isTemporary=True),\n",
       " Function(name='float', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='floor', description=None, className='org.apache.spark.sql.catalyst.expressions.FloorExpressionBuilder', isTemporary=True),\n",
       " Function(name='forall', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayForAll', isTemporary=True),\n",
       " Function(name='format_number', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatNumber', isTemporary=True),\n",
       " Function(name='format_string', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatString', isTemporary=True),\n",
       " Function(name='from_csv', description=None, className='org.apache.spark.sql.catalyst.expressions.CsvToStructs', isTemporary=True),\n",
       " Function(name='from_json', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonToStructs', isTemporary=True),\n",
       " Function(name='from_unixtime', description=None, className='org.apache.spark.sql.catalyst.expressions.FromUnixTime', isTemporary=True),\n",
       " Function(name='from_utc_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.FromUTCTimestamp', isTemporary=True),\n",
       " Function(name='get_json_object', description=None, className='org.apache.spark.sql.catalyst.expressions.GetJsonObject', isTemporary=True),\n",
       " Function(name='getbit', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseGet', isTemporary=True),\n",
       " Function(name='greatest', description=None, className='org.apache.spark.sql.catalyst.expressions.Greatest', isTemporary=True),\n",
       " Function(name='grouping', description=None, className='org.apache.spark.sql.catalyst.expressions.Grouping', isTemporary=True),\n",
       " Function(name='grouping_id', description=None, className='org.apache.spark.sql.catalyst.expressions.GroupingID', isTemporary=True),\n",
       " Function(name='hash', description=None, className='org.apache.spark.sql.catalyst.expressions.Murmur3Hash', isTemporary=True),\n",
       " Function(name='hex', description=None, className='org.apache.spark.sql.catalyst.expressions.Hex', isTemporary=True),\n",
       " Function(name='histogram_numeric', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.HistogramNumeric', isTemporary=True),\n",
       " Function(name='hour', description=None, className='org.apache.spark.sql.catalyst.expressions.Hour', isTemporary=True),\n",
       " Function(name='hypot', description=None, className='org.apache.spark.sql.catalyst.expressions.Hypot', isTemporary=True),\n",
       " Function(name='if', description=None, className='org.apache.spark.sql.catalyst.expressions.If', isTemporary=True),\n",
       " Function(name='ifnull', description=None, className='org.apache.spark.sql.catalyst.expressions.Nvl', isTemporary=True),\n",
       " Function(name='ilike', description=None, className='org.apache.spark.sql.catalyst.expressions.ILike', isTemporary=True),\n",
       " Function(name='in', description=None, className='org.apache.spark.sql.catalyst.expressions.In', isTemporary=True),\n",
       " Function(name='initcap', description=None, className='org.apache.spark.sql.catalyst.expressions.InitCap', isTemporary=True),\n",
       " Function(name='inline', description=None, className='org.apache.spark.sql.catalyst.expressions.Inline', isTemporary=True),\n",
       " Function(name='inline_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.Inline', isTemporary=True),\n",
       " Function(name='input_file_block_length', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileBlockLength', isTemporary=True),\n",
       " Function(name='input_file_block_start', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileBlockStart', isTemporary=True),\n",
       " Function(name='input_file_name', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileName', isTemporary=True),\n",
       " Function(name='instr', description=None, className='org.apache.spark.sql.catalyst.expressions.StringInstr', isTemporary=True),\n",
       " Function(name='int', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='isnan', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNaN', isTemporary=True),\n",
       " Function(name='isnotnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNotNull', isTemporary=True),\n",
       " Function(name='isnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNull', isTemporary=True),\n",
       " Function(name='java_method', description=None, className='org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection', isTemporary=True),\n",
       " Function(name='json_array_length', description=None, className='org.apache.spark.sql.catalyst.expressions.LengthOfJsonArray', isTemporary=True),\n",
       " Function(name='json_object_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonObjectKeys', isTemporary=True),\n",
       " Function(name='json_tuple', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonTuple', isTemporary=True),\n",
       " Function(name='kurtosis', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Kurtosis', isTemporary=True),\n",
       " Function(name='lag', description=None, className='org.apache.spark.sql.catalyst.expressions.Lag', isTemporary=True),\n",
       " Function(name='last', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Last', isTemporary=True),\n",
       " Function(name='last_day', description=None, className='org.apache.spark.sql.catalyst.expressions.LastDay', isTemporary=True),\n",
       " Function(name='last_value', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Last', isTemporary=True),\n",
       " Function(name='lcase', description=None, className='org.apache.spark.sql.catalyst.expressions.Lower', isTemporary=True),\n",
       " Function(name='lead', description=None, className='org.apache.spark.sql.catalyst.expressions.Lead', isTemporary=True),\n",
       " Function(name='least', description=None, className='org.apache.spark.sql.catalyst.expressions.Least', isTemporary=True),\n",
       " Function(name='left', description=None, className='org.apache.spark.sql.catalyst.expressions.Left', isTemporary=True),\n",
       " Function(name='length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
       " Function(name='levenshtein', description=None, className='org.apache.spark.sql.catalyst.expressions.Levenshtein', isTemporary=True),\n",
       " Function(name='like', description=None, className='org.apache.spark.sql.catalyst.expressions.Like', isTemporary=True),\n",
       " Function(name='ln', description=None, className='org.apache.spark.sql.catalyst.expressions.Log', isTemporary=True),\n",
       " Function(name='locate', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLocate', isTemporary=True),\n",
       " Function(name='log', description=None, className='org.apache.spark.sql.catalyst.expressions.Logarithm', isTemporary=True),\n",
       " Function(name='log10', description=None, className='org.apache.spark.sql.catalyst.expressions.Log10', isTemporary=True),\n",
       " Function(name='log1p', description=None, className='org.apache.spark.sql.catalyst.expressions.Log1p', isTemporary=True),\n",
       " Function(name='log2', description=None, className='org.apache.spark.sql.catalyst.expressions.Log2', isTemporary=True),\n",
       " Function(name='lower', description=None, className='org.apache.spark.sql.catalyst.expressions.Lower', isTemporary=True),\n",
       " Function(name='lpad', description=None, className='org.apache.spark.sql.catalyst.expressions.LPadExpressionBuilder', isTemporary=True),\n",
       " Function(name='ltrim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrimLeft', isTemporary=True),\n",
       " Function(name='make_date', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeDate', isTemporary=True),\n",
       " Function(name='make_dt_interval', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeDTInterval', isTemporary=True),\n",
       " Function(name='make_interval', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeInterval', isTemporary=True),\n",
       " Function(name='make_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeTimestamp', isTemporary=True),\n",
       " Function(name='make_ym_interval', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeYMInterval', isTemporary=True),\n",
       " Function(name='map', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateMap', isTemporary=True),\n",
       " Function(name='map_concat', description=None, className='org.apache.spark.sql.catalyst.expressions.MapConcat', isTemporary=True),\n",
       " Function(name='map_contains_key', description=None, className='org.apache.spark.sql.catalyst.expressions.MapContainsKey', isTemporary=True),\n",
       " Function(name='map_entries', description=None, className='org.apache.spark.sql.catalyst.expressions.MapEntries', isTemporary=True),\n",
       " Function(name='map_filter', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFilter', isTemporary=True),\n",
       " Function(name='map_from_arrays', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFromArrays', isTemporary=True),\n",
       " Function(name='map_from_entries', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFromEntries', isTemporary=True),\n",
       " Function(name='map_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.MapKeys', isTemporary=True),\n",
       " Function(name='map_values', description=None, className='org.apache.spark.sql.catalyst.expressions.MapValues', isTemporary=True),\n",
       " Function(name='map_zip_with', description=None, className='org.apache.spark.sql.catalyst.expressions.MapZipWith', isTemporary=True),\n",
       " Function(name='max', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Max', isTemporary=True),\n",
       " Function(name='max_by', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.MaxBy', isTemporary=True),\n",
       " Function(name='md5', description=None, className='org.apache.spark.sql.catalyst.expressions.Md5', isTemporary=True),\n",
       " Function(name='mean', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Average', isTemporary=True),\n",
       " Function(name='min', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Min', isTemporary=True),\n",
       " Function(name='min_by', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.MinBy', isTemporary=True),\n",
       " Function(name='minute', description=None, className='org.apache.spark.sql.catalyst.expressions.Minute', isTemporary=True),\n",
       " Function(name='mod', description=None, className='org.apache.spark.sql.catalyst.expressions.Remainder', isTemporary=True),\n",
       " Function(name='monotonically_increasing_id', description=None, className='org.apache.spark.sql.catalyst.expressions.MonotonicallyIncreasingID', isTemporary=True),\n",
       " Function(name='month', description=None, className='org.apache.spark.sql.catalyst.expressions.Month', isTemporary=True),\n",
       " Function(name='months_between', description=None, className='org.apache.spark.sql.catalyst.expressions.MonthsBetween', isTemporary=True),\n",
       " Function(name='named_struct', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateNamedStruct', isTemporary=True),\n",
       " Function(name='nanvl', description=None, className='org.apache.spark.sql.catalyst.expressions.NaNvl', isTemporary=True),\n",
       " Function(name='negative', description=None, className='org.apache.spark.sql.catalyst.expressions.UnaryMinus', isTemporary=True),\n",
       " Function(name='next_day', description=None, className='org.apache.spark.sql.catalyst.expressions.NextDay', isTemporary=True),\n",
       " Function(name='not', description=None, className='org.apache.spark.sql.catalyst.expressions.Not', isTemporary=True),\n",
       " Function(name='now', description=None, className='org.apache.spark.sql.catalyst.expressions.Now', isTemporary=True),\n",
       " Function(name='nth_value', description=None, className='org.apache.spark.sql.catalyst.expressions.NthValue', isTemporary=True),\n",
       " Function(name='ntile', description=None, className='org.apache.spark.sql.catalyst.expressions.NTile', isTemporary=True),\n",
       " Function(name='nullif', description=None, className='org.apache.spark.sql.catalyst.expressions.NullIf', isTemporary=True),\n",
       " Function(name='nvl', description=None, className='org.apache.spark.sql.catalyst.expressions.Nvl', isTemporary=True),\n",
       " Function(name='nvl2', description=None, className='org.apache.spark.sql.catalyst.expressions.Nvl2', isTemporary=True),\n",
       " Function(name='octet_length', description=None, className='org.apache.spark.sql.catalyst.expressions.OctetLength', isTemporary=True),\n",
       " Function(name='or', description=None, className='org.apache.spark.sql.catalyst.expressions.Or', isTemporary=True),\n",
       " Function(name='overlay', description=None, className='org.apache.spark.sql.catalyst.expressions.Overlay', isTemporary=True),\n",
       " Function(name='parse_url', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseUrl', isTemporary=True),\n",
       " Function(name='percent_rank', description=None, className='org.apache.spark.sql.catalyst.expressions.PercentRank', isTemporary=True),\n",
       " Function(name='percentile', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Percentile', isTemporary=True),\n",
       " Function(name='percentile_approx', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile', isTemporary=True),\n",
       " Function(name='pi', description=None, className='org.apache.spark.sql.catalyst.expressions.Pi', isTemporary=True),\n",
       " Function(name='pmod', description=None, className='org.apache.spark.sql.catalyst.expressions.Pmod', isTemporary=True),\n",
       " Function(name='posexplode', description=None, className='org.apache.spark.sql.catalyst.expressions.PosExplode', isTemporary=True),\n",
       " Function(name='posexplode_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.PosExplode', isTemporary=True),\n",
       " Function(name='position', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLocate', isTemporary=True),\n",
       " Function(name='positive', description=None, className='org.apache.spark.sql.catalyst.expressions.UnaryPositive', isTemporary=True),\n",
       " Function(name='pow', description=None, className='org.apache.spark.sql.catalyst.expressions.Pow', isTemporary=True),\n",
       " Function(name='power', description=None, className='org.apache.spark.sql.catalyst.expressions.Pow', isTemporary=True),\n",
       " Function(name='printf', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatString', isTemporary=True),\n",
       " Function(name='quarter', description=None, className='org.apache.spark.sql.catalyst.expressions.Quarter', isTemporary=True),\n",
       " Function(name='radians', description=None, className='org.apache.spark.sql.catalyst.expressions.ToRadians', isTemporary=True),\n",
       " Function(name='raise_error', description=None, className='org.apache.spark.sql.catalyst.expressions.RaiseError', isTemporary=True),\n",
       " Function(name='rand', description=None, className='org.apache.spark.sql.catalyst.expressions.Rand', isTemporary=True),\n",
       " Function(name='randn', description=None, className='org.apache.spark.sql.catalyst.expressions.Randn', isTemporary=True),\n",
       " Function(name='random', description=None, className='org.apache.spark.sql.catalyst.expressions.Rand', isTemporary=True),\n",
       " Function(name='range', description=None, className='org.apache.spark.sql.catalyst.plans.logical.Range', isTemporary=True),\n",
       " Function(name='rank', description=None, className='org.apache.spark.sql.catalyst.expressions.Rank', isTemporary=True),\n",
       " Function(name='reflect', description=None, className='org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection', isTemporary=True),\n",
       " Function(name='regexp', description=None, className='org.apache.spark.sql.catalyst.expressions.RLike', isTemporary=True),\n",
       " Function(name='regexp_extract', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpExtract', isTemporary=True),\n",
       " Function(name='regexp_extract_all', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpExtractAll', isTemporary=True),\n",
       " Function(name='regexp_like', description=None, className='org.apache.spark.sql.catalyst.expressions.RLike', isTemporary=True),\n",
       " Function(name='regexp_replace', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpReplace', isTemporary=True),\n",
       " Function(name='regr_avgx', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.RegrAvgX', isTemporary=True),\n",
       " Function(name='regr_avgy', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.RegrAvgY', isTemporary=True),\n",
       " Function(name='regr_count', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.RegrCount', isTemporary=True),\n",
       " Function(name='regr_r2', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.RegrR2', isTemporary=True),\n",
       " Function(name='repeat', description=None, className='org.apache.spark.sql.catalyst.expressions.StringRepeat', isTemporary=True),\n",
       " Function(name='replace', description=None, className='org.apache.spark.sql.catalyst.expressions.StringReplace', isTemporary=True),\n",
       " Function(name='reverse', description=None, className='org.apache.spark.sql.catalyst.expressions.Reverse', isTemporary=True),\n",
       " Function(name='right', description=None, className='org.apache.spark.sql.catalyst.expressions.Right', isTemporary=True),\n",
       " Function(name='rint', description=None, className='org.apache.spark.sql.catalyst.expressions.Rint', isTemporary=True),\n",
       " Function(name='rlike', description=None, className='org.apache.spark.sql.catalyst.expressions.RLike', isTemporary=True),\n",
       " Function(name='round', description=None, className='org.apache.spark.sql.catalyst.expressions.Round', isTemporary=True),\n",
       " Function(name='row_number', description=None, className='org.apache.spark.sql.catalyst.expressions.RowNumber', isTemporary=True),\n",
       " Function(name='rpad', description=None, className='org.apache.spark.sql.catalyst.expressions.RPadExpressionBuilder', isTemporary=True),\n",
       " Function(name='rtrim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrimRight', isTemporary=True),\n",
       " Function(name='schema_of_csv', description=None, className='org.apache.spark.sql.catalyst.expressions.SchemaOfCsv', isTemporary=True),\n",
       " Function(name='schema_of_json', description=None, className='org.apache.spark.sql.catalyst.expressions.SchemaOfJson', isTemporary=True),\n",
       " Function(name='sec', description=None, className='org.apache.spark.sql.catalyst.expressions.Sec', isTemporary=True),\n",
       " Function(name='second', description=None, className='org.apache.spark.sql.catalyst.expressions.Second', isTemporary=True),\n",
       " Function(name='sentences', description=None, className='org.apache.spark.sql.catalyst.expressions.Sentences', isTemporary=True),\n",
       " Function(name='sequence', description=None, className='org.apache.spark.sql.catalyst.expressions.Sequence', isTemporary=True),\n",
       " Function(name='session_window', description=None, className='org.apache.spark.sql.catalyst.expressions.SessionWindow', isTemporary=True),\n",
       " Function(name='sha', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha1', isTemporary=True),\n",
       " Function(name='sha1', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha1', isTemporary=True),\n",
       " Function(name='sha2', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha2', isTemporary=True),\n",
       " Function(name='shiftleft', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftLeft', isTemporary=True),\n",
       " Function(name='shiftright', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftRight', isTemporary=True),\n",
       " Function(name='shiftrightunsigned', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftRightUnsigned', isTemporary=True),\n",
       " Function(name='shuffle', description=None, className='org.apache.spark.sql.catalyst.expressions.Shuffle', isTemporary=True),\n",
       " Function(name='sign', description=None, className='org.apache.spark.sql.catalyst.expressions.Signum', isTemporary=True),\n",
       " Function(name='signum', description=None, className='org.apache.spark.sql.catalyst.expressions.Signum', isTemporary=True),\n",
       " Function(name='sin', description=None, className='org.apache.spark.sql.catalyst.expressions.Sin', isTemporary=True),\n",
       " Function(name='sinh', description=None, className='org.apache.spark.sql.catalyst.expressions.Sinh', isTemporary=True),\n",
       " Function(name='size', description=None, className='org.apache.spark.sql.catalyst.expressions.Size', isTemporary=True),\n",
       " Function(name='skewness', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Skewness', isTemporary=True),\n",
       " Function(name='slice', description=None, className='org.apache.spark.sql.catalyst.expressions.Slice', isTemporary=True),\n",
       " Function(name='smallint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='some', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True),\n",
       " Function(name='sort_array', description=None, className='org.apache.spark.sql.catalyst.expressions.SortArray', isTemporary=True),\n",
       " Function(name='soundex', description=None, className='org.apache.spark.sql.catalyst.expressions.SoundEx', isTemporary=True),\n",
       " Function(name='space', description=None, className='org.apache.spark.sql.catalyst.expressions.StringSpace', isTemporary=True),\n",
       " Function(name='spark_partition_id', description=None, className='org.apache.spark.sql.catalyst.expressions.SparkPartitionID', isTemporary=True),\n",
       " Function(name='split', description=None, className='org.apache.spark.sql.catalyst.expressions.StringSplit', isTemporary=True),\n",
       " Function(name='split_part', description=None, className='org.apache.spark.sql.catalyst.expressions.SplitPart', isTemporary=True),\n",
       " Function(name='sqrt', description=None, className='org.apache.spark.sql.catalyst.expressions.Sqrt', isTemporary=True),\n",
       " Function(name='stack', description=None, className='org.apache.spark.sql.catalyst.expressions.Stack', isTemporary=True),\n",
       " Function(name='startswith', description=None, className='org.apache.spark.sql.catalyst.expressions.StartsWithExpressionBuilder', isTemporary=True),\n",
       " Function(name='std', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
       " Function(name='stddev', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
       " Function(name='stddev_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevPop', isTemporary=True),\n",
       " Function(name='stddev_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
       " Function(name='str_to_map', description=None, className='org.apache.spark.sql.catalyst.expressions.StringToMap', isTemporary=True),\n",
       " Function(name='string', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='struct', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateNamedStruct', isTemporary=True),\n",
       " Function(name='substr', description=None, className='org.apache.spark.sql.catalyst.expressions.Substring', isTemporary=True),\n",
       " Function(name='substring', description=None, className='org.apache.spark.sql.catalyst.expressions.Substring', isTemporary=True),\n",
       " Function(name='substring_index', description=None, className='org.apache.spark.sql.catalyst.expressions.SubstringIndex', isTemporary=True),\n",
       " Function(name='sum', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Sum', isTemporary=True),\n",
       " Function(name='tan', description=None, className='org.apache.spark.sql.catalyst.expressions.Tan', isTemporary=True),\n",
       " Function(name='tanh', description=None, className='org.apache.spark.sql.catalyst.expressions.Tanh', isTemporary=True),\n",
       " Function(name='timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='timestamp_micros', description=None, className='org.apache.spark.sql.catalyst.expressions.MicrosToTimestamp', isTemporary=True),\n",
       " Function(name='timestamp_millis', description=None, className='org.apache.spark.sql.catalyst.expressions.MillisToTimestamp', isTemporary=True),\n",
       " Function(name='timestamp_seconds', description=None, className='org.apache.spark.sql.catalyst.expressions.SecondsToTimestamp', isTemporary=True),\n",
       " Function(name='tinyint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='to_binary', description=None, className='org.apache.spark.sql.catalyst.expressions.ToBinary', isTemporary=True),\n",
       " Function(name='to_csv', description=None, className='org.apache.spark.sql.catalyst.expressions.StructsToCsv', isTemporary=True),\n",
       " Function(name='to_date', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseToDate', isTemporary=True),\n",
       " Function(name='to_json', description=None, className='org.apache.spark.sql.catalyst.expressions.StructsToJson', isTemporary=True),\n",
       " Function(name='to_number', description=None, className='org.apache.spark.sql.catalyst.expressions.ToNumber', isTemporary=True),\n",
       " Function(name='to_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseToTimestamp', isTemporary=True),\n",
       " Function(name='to_unix_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ToUnixTimestamp', isTemporary=True),\n",
       " Function(name='to_utc_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ToUTCTimestamp', isTemporary=True),\n",
       " Function(name='transform', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayTransform', isTemporary=True),\n",
       " Function(name='transform_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.TransformKeys', isTemporary=True),\n",
       " Function(name='transform_values', description=None, className='org.apache.spark.sql.catalyst.expressions.TransformValues', isTemporary=True),\n",
       " Function(name='translate', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTranslate', isTemporary=True),\n",
       " Function(name='trim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrim', isTemporary=True),\n",
       " Function(name='trunc', description=None, className='org.apache.spark.sql.catalyst.expressions.TruncDate', isTemporary=True),\n",
       " Function(name='try_add', description=None, className='org.apache.spark.sql.catalyst.expressions.TryAdd', isTemporary=True),\n",
       " Function(name='try_avg', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.TryAverage', isTemporary=True),\n",
       " Function(name='try_divide', description=None, className='org.apache.spark.sql.catalyst.expressions.TryDivide', isTemporary=True),\n",
       " Function(name='try_element_at', description=None, className='org.apache.spark.sql.catalyst.expressions.TryElementAt', isTemporary=True),\n",
       " Function(name='try_multiply', description=None, className='org.apache.spark.sql.catalyst.expressions.TryMultiply', isTemporary=True),\n",
       " Function(name='try_subtract', description=None, className='org.apache.spark.sql.catalyst.expressions.TrySubtract', isTemporary=True),\n",
       " Function(name='try_sum', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.TrySum', isTemporary=True),\n",
       " Function(name='try_to_binary', description=None, className='org.apache.spark.sql.catalyst.expressions.TryToBinary', isTemporary=True),\n",
       " Function(name='try_to_number', description=None, className='org.apache.spark.sql.catalyst.expressions.TryToNumber', isTemporary=True),\n",
       " Function(name='typeof', description=None, className='org.apache.spark.sql.catalyst.expressions.TypeOf', isTemporary=True),\n",
       " Function(name='ucase', description=None, className='org.apache.spark.sql.catalyst.expressions.Upper', isTemporary=True),\n",
       " Function(name='unbase64', description=None, className='org.apache.spark.sql.catalyst.expressions.UnBase64', isTemporary=True),\n",
       " Function(name='unhex', description=None, className='org.apache.spark.sql.catalyst.expressions.Unhex', isTemporary=True),\n",
       " Function(name='unix_date', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixDate', isTemporary=True),\n",
       " Function(name='unix_micros', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixMicros', isTemporary=True),\n",
       " Function(name='unix_millis', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixMillis', isTemporary=True),\n",
       " Function(name='unix_seconds', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixSeconds', isTemporary=True),\n",
       " Function(name='unix_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixTimestamp', isTemporary=True),\n",
       " Function(name='upper', description=None, className='org.apache.spark.sql.catalyst.expressions.Upper', isTemporary=True),\n",
       " Function(name='uuid', description=None, className='org.apache.spark.sql.catalyst.expressions.Uuid', isTemporary=True),\n",
       " Function(name='var_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VariancePop', isTemporary=True),\n",
       " Function(name='var_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp', isTemporary=True),\n",
       " Function(name='variance', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp', isTemporary=True),\n",
       " Function(name='version', description=None, className='org.apache.spark.sql.catalyst.expressions.SparkVersion', isTemporary=True),\n",
       " Function(name='weekday', description=None, className='org.apache.spark.sql.catalyst.expressions.WeekDay', isTemporary=True),\n",
       " Function(name='weekofyear', description=None, className='org.apache.spark.sql.catalyst.expressions.WeekOfYear', isTemporary=True),\n",
       " Function(name='when', description=None, className='org.apache.spark.sql.catalyst.expressions.CaseWhen', isTemporary=True),\n",
       " Function(name='width_bucket', description=None, className='org.apache.spark.sql.catalyst.expressions.WidthBucket', isTemporary=True),\n",
       " Function(name='window', description=None, className='org.apache.spark.sql.catalyst.expressions.TimeWindow', isTemporary=True),\n",
       " Function(name='xpath', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathList', isTemporary=True),\n",
       " Function(name='xpath_boolean', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathBoolean', isTemporary=True),\n",
       " Function(name='xpath_double', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathDouble', isTemporary=True),\n",
       " Function(name='xpath_float', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathFloat', isTemporary=True),\n",
       " Function(name='xpath_int', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathInt', isTemporary=True),\n",
       " Function(name='xpath_long', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathLong', isTemporary=True),\n",
       " Function(name='xpath_number', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathDouble', isTemporary=True),\n",
       " Function(name='xpath_short', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathShort', isTemporary=True),\n",
       " Function(name='xpath_string', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathString', isTemporary=True),\n",
       " Function(name='xxhash64', description=None, className='org.apache.spark.sql.catalyst.expressions.XxHash64', isTemporary=True),\n",
       " Function(name='year', description=None, className='org.apache.spark.sql.catalyst.expressions.Year', isTemporary=True),\n",
       " Function(name='zip_with', description=None, className='org.apache.spark.sql.catalyst.expressions.ZipWith', isTemporary=True),\n",
       " Function(name='|', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseOr', isTemporary=True),\n",
       " Function(name='~', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseNot', isTemporary=True)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#devolvera funciones\n",
    "spark.catalog.listFunctions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#obtenemos base de datos actual\n",
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#almacenamos cache\n",
    "spark.catalog.cacheTable(\"default.hive_empdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#comprobamos si esta almacenada\n",
    "spark.catalog.isCached(\"default.hive_empdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#descacheamos una tabla\n",
    "spark.catalog.uncacheTable(\"default.hive_empdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify uncached table. Now you will see that it will return \"False\" which means table is not cached.\n",
    "spark.catalog.isCached(\"default.hive_empdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descachear toda la tabla en la sesion de spark\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
